{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation, Modeling and Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll look at how to transform the input datasets to be consumed into a Data Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - What inputs do you have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Get your tools setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of this we will be using [PySpark](http://spark.apache.org/docs/2.1.1/api/python/index.html) to inspect the data on hand and also gather some basic details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "\n",
    "#Locating where pyspark is installed\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "#Settings for PySpark to work\n",
    "driver_memory = '4g'\n",
    "num_executors = 2\n",
    "executor_memory = '1g'\n",
    "#pyspark_submit_args = ' --driver-memory ' + driver_memory + ' --executor-memory ' + executor_memory + ' --num-executors ' + num_executors + ' pyspark-shell'\n",
    "pyspark_submit_args = ' --driver-memory ' + driver_memory + ' pyspark-shell'\n",
    "\n",
    "#Setting the required parameters to start up PySpark\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n",
    "\n",
    "#Import Modules Needed for PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper for pretty formatting for Spark DataFrames\n",
    "def showDF(df, limitRows =  20, truncate = True):\n",
    "    if(truncate):\n",
    "        pd.set_option('display.max_colwidth', 50)\n",
    "    else:\n",
    "        pd.set_option('display.max_colwidth', -1)\n",
    "    pd.set_option('display.max_rows', limitRows)\n",
    "    display(df.limit(limitRows).toPandas())\n",
    "    pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a spark session\n",
    "spark = SparkSession.builder.appName(\"Data Transformation\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Read the data\n",
    "\n",
    "We will be using the WDICountry, WDISeries and WDIData files for all the activity below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the file into a Spark Data Frame\n",
    "country = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"../data/WDICountry.csv\")\n",
    "\n",
    "country.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"../data/WDISeries.csv\")\n",
    "\n",
    "series.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"../data/WDIData.csv\")\n",
    "\n",
    "indicators.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Transform the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the Country Dataset\n",
    "- Select the columns that we will need for our data model\n",
    "- Rename columns for data ingestion\n",
    "\n",
    "We'll be using the various operations supported for a DataFrame.  You can view the complete list [here](http://spark.apache.org/docs/2.1.1/api/python/pyspark.sql.html#pyspark.sql.DataFrame) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryDim = country \\\n",
    "    .select(\"2-alpha code\", \"Country Code\", \"Short Name\", \"Long Name\", \"Region\", \"Income Group\") \\\n",
    "    .withColumnRenamed(\"2-alpha code\", \"country_iso_code\") \\\n",
    "    .withColumnRenamed(\"Country Code\", \"wb_country_code\") \\\n",
    "    .withColumnRenamed(\"Short Name\", \"country_name\") \\\n",
    "    .withColumnRenamed(\"Long Name\", \"country_long_name\") \\\n",
    "    .withColumnRenamed(\"Region\", \"region\") \\\n",
    "    .withColumnRenamed(\"Income Group\", \"income_group\")\n",
    "    \n",
    "showDF(countryDim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can also do the similar transformations in sql\n",
    "Let's rename the country_name and country_long_name columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets you create a view that you can use in SQL queries\n",
    "countryDim.createOrReplaceTempView(\"country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformQuery = \"\"\"\n",
    "select \n",
    "    country_iso_code\n",
    "    , wb_country_code\n",
    "    , country_name as name\n",
    "    , country_long_name as long_name\n",
    "    , region\n",
    "    , income_group\n",
    "from \n",
    "    country\n",
    "\"\"\"\n",
    "\n",
    "showDF(spark.sql(transformQuery))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Check the data quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Do all countries have 2 character country_iso_codes ?\n",
    "\n",
    "We will use some sql functions supported in PySpark for this exercise.  You can find a complete list of functions supported [here](http://spark.apache.org/docs/2.1.1/api/python/pyspark.sql.html#module-pyspark.sql.functions) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showDF( \\\n",
    "    countryDim \\\n",
    "       .select(col(\"country_iso_code\"), \\\n",
    "               length(col(\"country_iso_code\")).alias(\"column_length\")) \\\n",
    "       .groupBy(\"column_length\") \\\n",
    "       .agg(count(\"*\").alias(\"cnt\")) \\\n",
    "       .filter(\"cnt > 1\") \\\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can also do this in SQL\n",
    "countryCodeLengthQuery = \"\"\"\n",
    "select \n",
    "    length(country_iso_code) as column_length\n",
    "    , count(1) as cnt\n",
    "from \n",
    "    country\n",
    "group by \n",
    "    length(country_iso_code)\n",
    "having \n",
    "    count(1) > 1\n",
    "\"\"\"\n",
    "\n",
    "showDF(spark.sql(countryCodeLengthQuery))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Exercise\n",
    "Do you notice a difference in the counts returned by the Quality check for the 2-character country_iso_code vs the original row count for the WDICountry.csv ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Do we have duplicate records for any of the key columns ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showDF(countryDim.groupBy(\"country_iso_code\").agg(count(\"*\").alias(\"cnt\")).filter(\"cnt > 1\"))\n",
    "\n",
    "showDF(countryDim.groupBy(\"wb_country_code\").agg(count(\"*\").alias(\"cnt\")).filter(\"cnt > 1\"))\n",
    "\n",
    "showDF(countryDim.groupBy(\"country_name\").agg(count(\"*\").alias(\"cnt\")).filter(\"cnt > 1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 - Fix any issues with data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryDimFinal = countryDim.filter(\"country_iso_code is not null\")\n",
    "\n",
    "showDF(countryDimFinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryDimFinal.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7 - Write the data to the destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are going to write the country dimension to an output csv file\n",
    "countryDimFinal \\\n",
    "    .coalesce(1) \\\n",
    "    .write.csv('../output/CountryDim', mode='overwrite', header='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8 - Check if the output was written out as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat ../output/CountryDim/*csv | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "#### Transform the Series Dataset and make it available in the dataframe seriesDim\n",
    "- Filter only for series that have Annual periodicity\n",
    "- Get the following columns and rename the selected columns to prepare further processing  \n",
    "\n",
    "| Name in CSV | Column Name |\n",
    "| ------------- |:-------------:|\n",
    "| Series Code | indicator_code |\n",
    "| Indicator Name | indicator_name |\n",
    "| Periodicity | periodicity |\n",
    "| Aggregation Method | aggregation_method |\n",
    "\n",
    "\n",
    "##### How many series do you end up with ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution here\n",
    "#seriesDim = ??? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What data quality checks were you able to perform ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Are there any duplicate codes ?\n",
    "- Are all indicator_codes following the same pattern ?\n",
    "- Is the case on columns consistent ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Transformations\n",
    "##### Problem Statement\n",
    "We want to measure the cellular and broadband penetration in comparison to the population demographics for every country.  It'll also be helpful to get some insights on annual global aggregates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Our dataset has multiple types of metrics.  The only ones that we care about are simple aggregates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleAggIndicators = seriesDim \\\n",
    "    .filter(\"lower(aggregation_method) = 'sum'\") \\\n",
    "    .select(\"indicator_code\", \"indicator_name\") \\\n",
    "    .orderBy(\"indicator_code\")\n",
    "\n",
    "showDF(simpleAggIndicators, limitRows = 500, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Only keep the indicators that are relevant to requirements i.e. Population indicators and Cellular and Broadband penetration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetIndicators = simpleAggIndicators \\\n",
    "    .filter(\"lower(indicator_name) like '%population%total%' \" + \n",
    "            \" or lower(indicator_name) like '%cellular%' \" +\n",
    "            \" or lower(indicator_name) like '%broadband%'\") \\\n",
    "    .filter(\"lower(indicator_name) not like '%refugee%'\")\n",
    "\n",
    "showDF(targetIndicators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now that we have identified the various indicators of interest, we can continue with getting the metrics for these indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the columns that are relevant for further transformations\n",
    "indicatorsData = indicators \\\n",
    "    .withColumnRenamed(\"Indicator Code\", \"indicator_code\") \\\n",
    "    .withColumnRenamed(\"Country Code\", \"wb_country_code\") \\\n",
    "    .drop(\"Indicator Name\") \\\n",
    "    .drop(\"Country Name\") \\\n",
    "    .drop(\"_c62\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep only the indicators that we care about\n",
    "targetIndicatorsData = indicatorsData.join(targetIndicators \\\n",
    "                                         , indicatorsData.indicator_code == targetIndicators.indicator_code) \\\n",
    "    .drop(targetIndicators.indicator_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showDF(targetIndicatorsData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The output that we see currently isn't the most ideal from a modelling perspective.  \n",
    "A well modeled dataset, should allow for data to be easily augmented.  e.g. instead of having a column for each year we would ideally prefer a row for each year to be able to add more rows in the future similar to the output of the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicatorsSample = targetIndicatorsData \\\n",
    "    .select(col(\"wb_country_code\")\n",
    "            , col(\"indicator_code\")\n",
    "            , lit(\"1960\").alias(\"year\")\n",
    "            , col(\"1960\").alias(\"indicator_value\")) \\\n",
    "    .filter(\"indicator_value >= 0.0\")\n",
    "\n",
    "showDF(indicatorsSample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let us start by getting the list of years that we have metrics for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearList = [x for x in targetIndicatorsData.schema.names \\\n",
    "             if x != 'wb_country_code' and x != 'indicator_code' and x != 'indicator_name'] \n",
    "\n",
    "print(yearList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cheat for creating a dataframe with no rows \n",
    "indicatorsDF = indicatorsSample.filter('1 = 0')\n",
    "\n",
    "#Iterate through the list of years and store the rows in the DataFrame we created above\n",
    "for indicatorYear in yearList:\n",
    "    print(\"Processing indicators for \" + indicatorYear)\n",
    "    yearIndicatorDF = targetIndicatorsData \\\n",
    "        .select(col(\"wb_country_code\")\n",
    "                , col(\"indicator_code\")\n",
    "                , lit(indicatorYear).alias(\"year\")\n",
    "                , col(indicatorYear).alias(\"indicator_value\")) \\\n",
    "        .filter(\"indicator_value >= 0\")\n",
    "    indicatorsDF = indicatorsDF.union(yearIndicatorDF)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's cache the dataset to iterate over it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can iterate over a dataframe that is already computed by caching it onces and using it repeatedly\n",
    "indicatorsDF.cache()\n",
    "\n",
    "#Force the data to be cached\n",
    "indicatorsDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the indicator counts per year\n",
    "#showDF(indicatorsDF.groupBy('year').agg(count(\"*\")).orderBy(\"year\"), limitRows=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting yearly indicator totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearPivot = indicatorsDF.groupBy('year').pivot('indicator_code').sum('indicator_value') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showDF(yearPivot.orderBy('year'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearPivot.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearPivotDF = yearPivot.orderBy('year') \\\n",
    "    .withColumnRenamed('IT.CEL.SETS', 'cellular_subscriptions') \\\n",
    "    .withColumnRenamed('IT.NET.BBND', 'broadband_subscriptions') \\\n",
    "    .withColumnRenamed('SP.POP.0014.TO', 'population_age_0_to_14') \\\n",
    "    .withColumnRenamed('SP.POP.1564.TO', 'population_age_15_64') \\\n",
    "    .withColumnRenamed('SP.POP.65UP.TO', 'population_age_65_and_above') \\\n",
    "    .withColumnRenamed('SP.POP.TOTL', 'population')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Quality Checkpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can iterate over a dataframe that is already computed by caching it onces and using it repeatedly\n",
    "yearPivotDF.cache()\n",
    "\n",
    "#Forces the data to be cached\n",
    "yearPivotDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearPivotDF.filter('population_age_0_to_14 < 0').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearPivotDF.filter('population_age_15_64 < 0').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearPivotDF.filter('population_age_0_to_14 < 0').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearPivotDF.filter('population_age_65_and_above < 0').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearPivotDF.filter('population < 0').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearPivotDF.filter('cellular_subscriptions < 0').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearPivotDF.filter('broadband_subscriptions < 0').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearPivotDF.filter('population_age_0_to_14 > population').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearPivotDF.filter('population_age_15_64 > population').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearPivotDF.filter('population_age_65_and_above > population').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearPivotDF.filter('(population_age_0_to_14 + population_age_15_64 + population_age_65_and_above) > population').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the yearly totals to a CSV File\n",
    "yearPivotDF \\\n",
    "    .select(col('year')\n",
    "            , col('population').cast(DecimalType(38, 2))\n",
    "            , col('population_age_0_to_14').cast(DecimalType(38, 2))\n",
    "            , col('population_age_15_64').cast(DecimalType(38, 2))\n",
    "            , col('population_age_65_and_above').cast(DecimalType(38, 2))\n",
    "            , col('broadband_subscriptions').cast(DecimalType(38, 2))\n",
    "            , col('cellular_subscriptions').cast(DecimalType(38, 2))) \\\n",
    "    .coalesce(1) \\\n",
    "    .write.csv('../output/YearlyStats', mode='overwrite', header='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting yearly regional totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionalIndicators = indicatorsDF.join(countryDimFinal\n",
    "                                       , indicatorsDF.wb_country_code == countryDim.wb_country_code\n",
    "                                       , \"inner\") \\\n",
    "    .select(countryDim.region\n",
    "            , indicatorsDF.wb_country_code\n",
    "            , indicatorsDF.year\n",
    "            , indicatorsDF.indicator_code\n",
    "            , indicatorsDF.indicator_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showDF(regionalIndicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionalPivot = regionalIndicators.groupBy('region', 'year').pivot('indicator_code').sum('indicator_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showDF(regionalPivot.orderBy('region', 'year'), limitRows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the regional-yearly totals to a CSV File\n",
    "regionalPivot.filter('region is not null') \\\n",
    "    .orderBy('region','year') \\\n",
    "    .withColumnRenamed('IT.CEL.SETS', 'cellular_subscriptions') \\\n",
    "    .withColumnRenamed('IT.NET.BBND', 'broadband_subscriptions') \\\n",
    "    .withColumnRenamed('SP.POP.0014.TO', 'population_age_0_to_14') \\\n",
    "    .withColumnRenamed('SP.POP.1564.TO', 'population_age_15_64') \\\n",
    "    .withColumnRenamed('SP.POP.65UP.TO', 'population_age_65_and_above') \\\n",
    "    .withColumnRenamed('SP.POP.TOTL', 'population') \\\n",
    "    .select(col('region')\n",
    "            , col('year')\n",
    "            , col('population').cast(DecimalType(38, 2))\n",
    "            , col('population_age_0_to_14').cast(DecimalType(38, 2))\n",
    "            , col('population_age_15_64').cast(DecimalType(38, 2))\n",
    "            , col('population_age_65_and_above').cast(DecimalType(38, 2))\n",
    "            , col('broadband_subscriptions').cast(DecimalType(38, 2))\n",
    "            , col('cellular_subscriptions').cast(DecimalType(38, 2))) \\\n",
    "    .coalesce(1) \\\n",
    "    .write.csv('../output/RegionalStats', mode='overwrite', header='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short Exercise\n",
    "\n",
    "Becky finds the regional metrics interesting, but she wants to look at these metrics at a country level for each year.  Can you adapt the regional pivot that we computed earlier to get the metrics for each year and country ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Kat wants to identify the countries that are conducive to start a business.  She thinks that it would suffice to look at the most recent metrics for the following:\n",
    "- Gross National Income (GNI)\n",
    "- Cost of business start-up procedures\n",
    "- Number of days required to start a business\n",
    "- Number of start-up procedures to register a business\n",
    "- GDP\n",
    "- GDP per capita\n",
    "- Business Regulatory Environment\n",
    "- Ease of doing business index (Only available in 2017)\n",
    "\n",
    "Write the data to a csv file.\n",
    "\n",
    "Hint - Start by matching up indicators that might have descriptions matching what Kat is looking for. e.g. <br>\n",
    "`showDF(seriesDim.filter(\"indicator_name like '%GNI%'\").orderBy(\"indicator_code\"), limitRows = 500)`"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
